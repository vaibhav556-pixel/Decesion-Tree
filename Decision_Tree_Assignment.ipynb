{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9f605e12",
      "metadata": {
        "id": "9f605e12"
      },
      "source": [
        "# Decision Tree — Assignment Answers\n",
        "\n",
        "### Question 1 — What is a Decision Tree, and how does it work in the context of classification?\n",
        "\n",
        "**Answer (short):**  \n",
        "A **Decision Tree** is a tree-structured model used for classification and regression. In classification, the tree splits the feature space into regions using simple decision rules (e.g., feature <= threshold). Each internal node tests one feature, branches represent outcomes of the test, and each leaf node assigns a class label (or class probabilities). The tree is grown by selecting splits that maximize a purity metric (e.g., Information Gain / Entropy or Gini Impurity) on the training data.\n",
        "\n",
        "**How it works (mechanics):**\n",
        "1. Start with the root node containing the full training set.\n",
        "2. For each candidate feature and threshold, compute an impurity measure for the resulting child nodes.\n",
        "3. Select the split that yields the largest decrease in impurity (highest information gain).\n",
        "4. Recursively split child nodes until a stopping criterion is met (pure nodes, max depth, min samples).\n",
        "5. For prediction, pass a sample down the tree following the tests until a leaf is reached; the leaf's class is the predicted class (or class distribution).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b61a8d1c",
      "metadata": {
        "id": "b61a8d1c"
      },
      "source": [
        "### Question 2 — Gini Impurity and Entropy (impurity measures)\n",
        "\n",
        "**Gini Impurity:**  \n",
        "- For a node with class probabilities \\(p_k\\), Gini impurity is \\(G = 1 - \\sum_k p_k^2\\).  \n",
        "- It measures the probability of mislabeling a random sample from the node if it were randomly labeled according to the class distribution.\n",
        "\n",
        "**Entropy:**  \n",
        "- Entropy is \\(H = -\\sum_k p_k \\log_2 p_k\\).  \n",
        "- It measures the amount of “disorder” or unpredictability in the class distribution.\n",
        "\n",
        "**Impact on splits:**  \n",
        "- Both measure impurity; splits are chosen to reduce impurity the most (maximize Information Gain).  \n",
        "- Entropy is typically more sensitive to changes in class probabilities near 0 and 1, while Gini is slightly faster to compute. In practice both often produce similar trees; Gini is the default in many implementations because of computational speed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40ab418a",
      "metadata": {
        "id": "40ab418a"
      },
      "source": [
        "### Question 3 — Pre-Pruning vs Post-Pruning\n",
        "\n",
        "**Pre-Pruning (early stopping):**  \n",
        "- Stop tree growth early using criteria such as `max_depth`, `min_samples_split`, `min_samples_leaf`, or a minimum impurity decrease.  \n",
        "- **Advantage:** Reduces overfitting and training time because tree never grows too large.\n",
        "\n",
        "**Post-Pruning (prune after full growth):**  \n",
        "- Grow a full tree and then remove (prune) branches that do not improve performance on a validation set (or using cost-complexity pruning).  \n",
        "- **Advantage:** Can produce simpler trees while keeping beneficial splits discovered in deep parts of the tree (often yields better accuracy than naive pre-pruning).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74c3f5d0",
      "metadata": {
        "id": "74c3f5d0"
      },
      "source": [
        "### Question 4 — Information Gain\n",
        "\n",
        "**Information Gain (IG):**  \n",
        "- IG measures the reduction in impurity (entropy or Gini) due to a split. For entropy:  \n",
        "  \\(\text{IG} = H(\text{parent}) - \\sum_{i \\in \\{\text{children}\\}}\n",
        "rac{N_i}{N}\\, H(i)\\).  \n",
        "- **Why important:** It quantifies how much a split improves class purity; the split with the highest IG is typically chosen as the best split.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf0bd62f",
      "metadata": {
        "id": "cf0bd62f"
      },
      "source": [
        "### Question 5 — Applications, advantages and limitations\n",
        "\n",
        "**Common applications:**  \n",
        "- Medical diagnosis, credit scoring, customer churn prediction, fraud detection, loan approval, and many tabular-data tasks.  \n",
        "- Also used inside ensemble methods like Random Forests and Gradient Boosted Trees.\n",
        "\n",
        "**Advantages:**  \n",
        "- Easy to interpret and visualize.  \n",
        "- Handles both numerical and categorical features.  \n",
        "- Requires little feature preparation (no scaling needed).  \n",
        "- Can model non-linear relationships and interactions.\n",
        "\n",
        "**Limitations:**  \n",
        "- Prone to overfitting if not pruned.  \n",
        "- Unstable: small changes in data can yield different trees.  \n",
        "- Single trees often underperform compared to ensembles (Random Forests, XGBoost) on predictive accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cb77de2e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb77de2e",
        "outputId": "81dac1c1-d4cb-4db1-d0d2-9ecec05da270"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Iris, Gini): 0.9333333333333333\n",
            "Feature importances:\n",
            "  sepal length (cm): 0.0062\n",
            "  sepal width (cm): 0.0292\n",
            "  petal length (cm): 0.5586\n",
            "  petal width (cm): 0.4060\n"
          ]
        }
      ],
      "source": [
        "# Question 6: Load Iris, train DecisionTreeClassifier with Gini, print accuracy and feature importances\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print('Accuracy (Iris, Gini):', acc)\n",
        "print('Feature importances:')\n",
        "for name, imp in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f'  {name}: {imp:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "daf74bc6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daf74bc6",
        "outputId": "6a60978b-489d-4f97-e3f0-0cec931994cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fully-grown tree accuracy: 0.9333\n",
            "max_depth=3 tree accuracy: 0.9667\n"
          ]
        }
      ],
      "source": [
        "# Question 7: Train DecisionTreeClassifier with max_depth=3 and compare to fully-grown tree\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# fully-grown tree (no max_depth)\n",
        "clf_full = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "acc_full = accuracy_score(y_test, clf_full.predict(X_test))\n",
        "\n",
        "# constrained tree\n",
        "clf_md3 = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
        "clf_md3.fit(X_train, y_train)\n",
        "acc_md3 = accuracy_score(y_test, clf_md3.predict(X_test))\n",
        "\n",
        "print(f'Fully-grown tree accuracy: {acc_full:.4f}')\n",
        "print(f'max_depth=3 tree accuracy: {acc_md3:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6deaa851",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6deaa851",
        "outputId": "51e6f181-8bd3-4793-bb13-28cc12842322"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset source: make_regression (fallback)\n",
            "MSE (DecisionTreeRegressor): 11399.504755366335\n",
            "\n",
            "Feature importances:\n",
            "  feat_0: 0.0077\n",
            "  feat_1: 0.0099\n",
            "  feat_2: 0.0065\n",
            "  feat_3: 0.0238\n",
            "  feat_4: 0.0673\n",
            "  feat_5: 0.0458\n",
            "  feat_6: 0.0686\n",
            "  feat_7: 0.0188\n",
            "  feat_8: 0.0327\n",
            "  feat_9: 0.3592\n",
            "  feat_10: 0.0055\n",
            "  feat_11: 0.0231\n",
            "  feat_12: 0.3310\n"
          ]
        }
      ],
      "source": [
        "# Question 8: Load Boston Housing Dataset (or fallback) and train DecisionTreeRegressor\n",
        "# Print MSE and feature importances\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Try to load the Boston dataset; if unavailable, create a synthetic regression dataset\n",
        "try:\n",
        "    # scikit-learn older versions provide load_boston\n",
        "    from sklearn.datasets import load_boston\n",
        "    bdata = load_boston()\n",
        "    Xb, yb = bdata.data, bdata.target\n",
        "    feature_names = list(bdata.feature_names)\n",
        "    source = 'load_boston()'\n",
        "except Exception:\n",
        "    # Fallback: create synthetic dataset with 13 features to mimic Boston's dimensionality\n",
        "    from sklearn.datasets import make_regression\n",
        "    Xb, yb = make_regression(n_samples=506, n_features=13, noise=0.5, random_state=42)\n",
        "    feature_names = [f'feat_{i}' for i in range(Xb.shape[1])]\n",
        "    source = 'make_regression (fallback)'\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "Xb_train, Xb_test, yb_train, yb_test = train_test_split(Xb, yb, test_size=0.2, random_state=42)\n",
        "\n",
        "reg = DecisionTreeRegressor(random_state=42)\n",
        "reg.fit(Xb_train, yb_train)\n",
        "yb_pred = reg.predict(Xb_test)\n",
        "mse = mean_squared_error(yb_test, yb_pred)\n",
        "\n",
        "print('Dataset source:', source)\n",
        "print('MSE (DecisionTreeRegressor):', mse)\n",
        "print('\\nFeature importances:')\n",
        "for name, imp in zip(feature_names, reg.feature_importances_):\n",
        "    print(f'  {name}: {imp:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c863c97c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c863c97c",
        "outputId": "4bff588a-834b-43b9-8f27-01385bae9f89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found: {'max_depth': None, 'min_samples_split': 2}\n",
            "Validation (cross-validated) score: 0.9416666666666668\n",
            "Test accuracy with best estimator: 0.9333333333333333\n"
          ]
        }
      ],
      "source": [
        "# Question 9: Tune max_depth and min_samples_split on Iris using GridSearchCV\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [None, 1, 2, 3, 4, 5],\n",
        "    'min_samples_split': [2, 3, 4, 5, 10]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print('Best parameters found:', grid.best_params_)\n",
        "best_clf = grid.best_estimator_\n",
        "print('Validation (cross-validated) score:', grid.best_score_)\n",
        "print('Test accuracy with best estimator:', accuracy_score(y_test, best_clf.predict(X_test)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c474d9f",
      "metadata": {
        "id": "1c474d9f"
      },
      "source": [
        "### Question 10 — Healthcare predictive model: step-by-step process\n",
        "\n",
        "**Problem:** Predict whether a patient has a disease. Dataset has mixed data types and missing values.\n",
        "\n",
        "**1) Handle missing values**\n",
        "- Inspect missingness pattern (missing completely at random vs not).  \n",
        "- If missingness is small: simple imputation — mean/median for numeric, most frequent or new category for categorical.  \n",
        "- If missingness is informative: add binary indicator columns for \"was_missing\".  \n",
        "- For advanced work: use `IterativeImputer` or model-based imputation.\n",
        "\n",
        "**2) Encode categorical features**\n",
        "- For low-cardinality categories: One-Hot Encoding (e.g., `OneHotEncoder`) or pandas `get_dummies`.  \n",
        "- For high-cardinality categories: Target encoding, frequency encoding, or embedding techniques.  \n",
        "- Ensure consistent encoding between train and production (save encoders).\n",
        "\n",
        "**3) Train a Decision Tree model**\n",
        "- Split data into train/validation/test (or use cross-validation).  \n",
        "- Optionally scale numeric data (not required for trees).  \n",
        "- Fit `DecisionTreeClassifier` using criterion (`gini` or `entropy`) and set basic regularization (`max_depth`, `min_samples_leaf`) to avoid overfitting.\n",
        "\n",
        "**4) Tune hyperparameters**\n",
        "- Use `GridSearchCV` or `RandomizedSearchCV` over `max_depth`, `min_samples_split`, `min_samples_leaf`, `max_features`, and `ccp_alpha` (cost-complexity pruning).  \n",
        "- Use cross-validation and consider class imbalance (use `class_weight='balanced'` or resampling).\n",
        "\n",
        "**5) Evaluate performance**\n",
        "- Use metrics appropriate to the business objective: accuracy, precision, recall, F1-score, ROC AUC. For medical diagnosis, maximize sensitivity/recall (catch cases) while maintaining acceptable precision.  \n",
        "- Calibrate predicted probabilities if needed (e.g., `CalibratedClassifierCV`).  \n",
        "- Use confusion matrix, ROC, PR curves, and calibration plots.\n",
        "- Perform fairness and bias checks and validate on hold-out / external dataset.\n",
        "\n",
        "**Business value**\n",
        "- Early identification of patients who may have the disease (triage), helping prioritize further testing and intervention.  \n",
        "- Better allocation of diagnostic resources and cost reduction by reducing unnecessary tests.  \n",
        "- Improved patient outcomes via earlier treatment, and potential to build clinical decision support tools (with careful validation and regulatory compliance).\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}